<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="index.css">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js"></script>
    <script src="index.js"></script>
</head>
<body>

<div class="container-fluid">           
<div class="row" style="height: 6rem; width:100%; background-color: #232F3E;">
    <div class="col-md-auto" style="line-height: 6rem;"><img src="images/aws_tiny_dark.png"></div>
    <div class="col-md-auto">
    <div style="color: #ffffff; font-size: 1.8rem; line-height: 6rem;" id="prerequisites">AWS Glue - Introduction</div>
    </div>
</div>

<h1>Prerequisites</h1>
<p>
If not already done, please complete the <a href="https://www.eventbox.dev/staging/lesson/event-engine-getting-started/index.html" target="_blank">Getting Started</a> lesson first. Next log in to <a href="https://eu-west-1.console.aws.amazon.com/cloud9/" target="_blank">AWS Cloud9</a>. The Python code for all labs, and all infrastructure code, is available via the <a href="https://github.com/sundgaard-aws/aws-glue.git" target="_blank">GitHub link</a> that was cloned earlier. Run the code below in a Cloud9 terminal.
</p>

<div class="code">
    cd aws-glue/iac/cdk<br>
    cdk deploy --all<br>
</div>

<p>You can roughly expect the following runtimes for the different stacks that are deployed as part of this process.</p>
<ul>
<li>Network Stack: 200 secs</li>
<li>Security Stack: 30 secs</li>
<li>Data Stack: 616 secs</li>
<li>Compute Stack: 57 secs</li>
</ul>

<h1 id="lab-0">Lab 0 - Setup</h1>
<p>
If not already done, start by logging in to <a href="https://eu-west-1.console.aws.amazon.com/cloud9/" target="_blank">AWS Cloud9</a>. Next we will ensure that the AWS credentials used are the right ones. After that we will start to copy some sample data to their respective S3 buckets. First let us ensure that the credentials are correct.
</p>

<div class="code">
    # ensure you are using Event Engine credentials<br/>
    aws sts get-caller-identity<br/>
</div>

<p>If the 'Role' contains the phrase 'TeamRole' then you are using the right credentials. You can continue using by running the code below, otherwise go back to the <a href="https://www.eventbox.dev/staging/lesson/event-engine-getting-started/index.html" target="_blank">Getting Started Guide</a>.</p>
<div class="code">
aws s3 ls | grep glue-input-<br/>
export tradeInputBucket=[paste-bucket-name-from-above]<br/>
echo $tradeInputBucket<br/>
cd ~/environment/git/aws-glue/python/trade-import<br/>
python generate-large-data-file.py<br/>
python generate-huge-data-file.py<br/>
echo copying generated files to S3 locations...<br/>
aws s3 cp ~/environment/git/aws-glue/sample-data/trade/fx-trades.json s3://$tradeInputBucket/<br/>
aws s3 cp ~/environment/git/aws-glue/sample-data/trade/fx-trades.csv s3://$tradeInputBucket/<br/>
aws s3 cp ~/environment/git/aws-glue/sample-data/trade/fx-trades-large.csv s3://$tradeInputBucket/<br/>
aws s3 cp ~/environment/git/aws-glue/sample-data/trade/fx-trades-huge.csv s3://$tradeInputBucket/<br/>
aws s3 ls | grep driver<br/>
export driverBucket=[paste-bucket-name-from-above]<br/>
echo $driverBucket<br/>
echo downloading mysql jdbc driver...<br/>
cd /tmp/<br/>
wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.25/mysql-connector-java-8.0.25.jar<br/>
echo copying jdbc driver to s3...<br/>
aws s3 cp mysql-connector-java-8.0.25.jar s3://$driverBucket/<br/>
echo done.<br/>
</div>

<p>Once all files have been copied to S3, verify that the files are in the buckets using either the AWS CLI or the <a href="https://eu-west-1.console.aws.amazon.com/s3/" target="_blank">Amazon S3 Console</a>.</p>

<h1 id="lab-1">Lab 1 - Creating a basic job</h1>
<p>
In this lab we will be running an &quot;Empty&quot; boilerplate job just with context and logging defined in the Python script. The job is already created for you by the CDK deployment that you run earlier. You can view and run it either via the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or the AWS CLI using <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">Cloud9</a>.</p>
<p>Try both to get a feeling of the pros and cons.<br/></p>

<div class="code">
aws glue list-jobs<br/>
aws glue start-job-run --job-name acc-day-glue-lab1<br/>
aws glue get-job-runs --job-name acc-day-glue-lab1 --output table<br/>
</div>

<p>Note the output logs in <a href="https://eu-west-1.console.aws.amazon.com/cloudwatch/" target="_blank">Amazon CloudWatch</a> as well as the timings for the run.</p>

<h1>Lab 2 - Read secret from AWS Secrets Manager</h1>
<p>
In this second lab we will be reading a database secret from AWS Secrets Manager. Navigate to <a href="https://eu-west-1.console.aws.amazon.com/secretsmanager/" target="_blank">AWS Secrets Manager</a> to view the secret as well as the secret values.
</p>

<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab2<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab2 --output table<br/>
</div>

<p>
Once the job has completed, open <a href="https://eu-west-1.console.aws.amazon.com/cloudwatch/" target="_blank">Amazon CloudWatch</a> and note that we are printing out the entire secret. This is obviously only viable for a temporary demo environment, and not something you would do in your own envionments. If the values are displayed correctly, we know that the the job has been configured correctly and that we have the right permissions defined for the execution role used.
</p>

<h1>Lab 3 - Read CSV data from Amazon S3</h1>
<p>
Now that we have the basics in place, we will actually start to read data from an S3 object store. AWS Glue has the concept of <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html" target="_blank">DynamicFrames</a> which is a more dynamic version of the more well known Python DataFrames. In most cases, reading data in to a DynamicFrame will be the first actual step in any Glue ETL job.
</p>

<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab3<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab3 --output table<br/>
</div>

<p>
    Once the job has completed, open <a href="https://eu-west-1.console.aws.amazon.com/cloudwatch/" target="_blank">Amazon CloudWatch</a> and note that we are printing out the contents of the data frame. This confirms that data is read correctly from the S3 bucket. Note that we are printing both the Glue dynamic frame, which will be printed as JSON and the converted Python data frame that will be printed as table text.
</p>

<h1>Lab 4 - Trim data in source data frame</h1>
<p>
Often source data comes in all shapes and sizes and can be filled with unexpected characters, whitespaces and much more. In the previous lab, did you notice some potential issues with the input file? Actually there were whitespaces in front of most of the values, which would cause problems as part of the type conversion process. You can verify this by looking at the JSON print out in <a href="https://eu-west-1.console.aws.amazon.com/cloudwatch/" target="_blank">Amazon CloudWatch</a>. In this lab we will make sure that all whitespaces are trimmed.
</p>

<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab4<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab4 --output table<br/>
</div>

<p>
    Once the job has completed, open <a href="https://eu-west-1.console.aws.amazon.com/cloudwatch/" target="_blank">Amazon CloudWatch</a> and note that again we are printing out the contents of the data frame. This time though we are printing out the trimmed frame as well. You should be able to see that the whitespace characters from the orignal frame are now gone.
</p>

<h1>Lab 5 - Apply mappings</h1>
<p>
    After reading the data we want to map it to the target schema. This is done using the apply() method of the <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-transforms-ApplyMapping.html" target="_blank">ApplyMapping</a> class. The ApplyMapping class is one of the AWS Glue classes in the <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-transforms.html" target="_blank">Transforms</a> namespace. Feel free to investigate the other classes in this namespace when time allows.
</p>

<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab5<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab5 --output table<br/>
</div>

<p>
    Once the job has completed, open <a href="https://eu-west-1.console.aws.amazon.com/cloudwatch/" target="_blank">Amazon CloudWatch</a> and verify that it has successfully printed the 'done applying mapping rules.' line.
</p>

<h1>Lab 6 - Write to target Amazon RDS MySQL DB</h1>
<p>
So far we have been reading data from Amazon S3, and we have trimmed/prepared and mapped the data to the target database schema. In this lab we will actually be writing the data to the target data store. We will be writing to Amazon RDS for MySQL, which is a managed MySQL compatible relational database offering from AWS.
</p>

<p>For performance reasons it may be a good idea to create a target table &quot;trade&quot; in the MySQL database. This will ensure that smaller 'varchar' columns are used over 'text' columns, and that a primary key is defined.</p>

<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab6<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab6 --output table<br/>
</div>

<p>
Once the job has completed, connect to the MySQL database instance, using the MySQL CLI from <a href="https://eu-west-1.console.aws.amazon.com/cloud9" target="_blank">Cloud9</a>. Please ensure that you have configured the <a href="https://eu-west-1.console.aws.amazon.com/cloud9" target="_blank">RDS security group</a> to allow inbound connections from Cloud9 for the MySQL port.
</p>

<p>
    Once the job has completed, connect to the MySQL database through the Cloud9 termnial. Make sure that the <a href="https://eu-west-1.console.aws.amazon.com/vpc/home?region=eu-west-1#securityGroups:" target="_blank">Secruity Group</a> for Cloud9 has access on the MySQL port for the Security Group for MySQL. If you forget this, your connection attempt will time out. Run the code below to connect to the database.<br/><br/>
    Remember to replace the values with the right information for your database instance. You can find these using <a href="https://eu-west-1.console.aws.amazon.com/secretsmanager" target="_blank">Secrets Manager</a>.
</p>

<div class="code">
    mysql -h [db-prefix].eu-west-1.rds.amazonaws.com -u [dbuser] -p[dbPW] tradedb<br/>
</div>

<p>The job will only insert a few records in this lab, so it is safe to select all contents from the table using the code below, in order to verify that the job works as excpected.</p>

<div class="code">
    select * from trade;<br/>
</div>

<h1>Lab 7 - Read JSON data from S3</h1>
<p>
    Reading JSON data from S3 is very similar to reading CSV data. The main difference in the code is the format parameter of the fromOptions() method which changes value from csv to json. A few other csv specific options are removed, and a few are added. We will not be preparing or mapping the data to anything in this lab, but we will simply read the data and investigate the CloudWatch logs.
</p>
<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab7<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab7 --output table<br/>
</div>

<p>
    Once the job has completed open <a href="https://eu-west-1.console.aws.amazon.com/cloudwatch/" target="_blank">Amazon CloudWatch</a> and notice the output from the DynamicFrame that has been read.
</p>

<h1>Lab 8 - Import large data set</h1>
<p>
Often when dealing with ETL jobs you will be working with thousands or even millions of rows. AWS Glue has many features to support large data sets. In the coming labs we will be focusing on some of those features and capabilities. Some are AWS Glue specific and other are related to the Python and PySpark language and framework.</p>
<p>
In this lab we will load 1M rows from a CSV file, and investigate the runtime metrics and logs produced by the job. This will give us an idea of the speed that you can expect from AWS Glue with the given settings. Also note the job details for the job, which specifies both the number of workers and the instance type used, being to of the most critical components when right sizing AWS Glue ETL jobs.
</p>

<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab8<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab8 --output table<br/>
</div>

<h1>Lab 9 - import huge data set (SKIP or let timeout)</h1>
Import large data set 20M+ rows.<br/>
Will most likely timeout as is.<br/>
Try to modify table.<br/>
Add Cloud9 to RDS security group inbound rule.<br/>
mysql -h .rds.amazonaws.com -u xx -pXX tradedb<br/>

<h1>Lab 10 - Writing to Amazon Dynamodb</h1>
<p>
For decades most databases have been relational databases or so called RDBMS. For the past years many new types of databases has emerged, including NoSQL and KeyValuePair databases. The ability to scale horizontally, as vertical scaling has limits and can be hard to right size, and the struggle to keep relational database schemas up to date and in sync with code and APIs are some of the main reasons why these database types are becoming more and more popular.
Write data to DynamoDB instead of MySQL
</p>

<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab10<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab10 --output table<br/>
</div>
<p>
    Verify that the trades has been written successfully to Amazon DynamoDB. Navigate to the <a href="https://eu-west-1.console.aws.amazon.com/cloudwatch/" target="_blank">DynamoDB Console</a>, and click "Explore Items".
</p>
<img src="images/lab10-1.png">
<img src="images/lab10-2.png">

<h1>Lab 11 - Write large data sets to Amazon Dynamodb</h1>
<p>
In this lab we will be writing a larger data set to Amazon DynamoDB. As DynamoDB is a serverless NoSQL database it will automatically scale to optimize for increased database activity. This means that the database should not be the bottleneck in this scenario.
</p>

<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab11<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab11 --output table<br/>
</div>

<p>After the job has completed, investigate the timings and logs in <a href="https://eu-west-1.console.aws.amazon.com/cloudwatch/" target="_blank">Amazon CloudWatch</a> to get a better feeling of where time was spent during this run.</p>

<h1>Lab 12 - Using multiple partitions</h1>
<p>
To optimize how we write to a data target, we can make use of paritions. Partitioning is a way of slicing your data in to logical groups, that can be handled separately or in parallel. By partitioning your data you can increase both read and write performance significantly. In this lab we will be focusing on write paritioning to speed up the database writes.
</p>
<p>With our current data set we only have one partition. We will force our data set to have more partitions by reshuffling the data in the DynamicFrame. We will not be writing to any data store in this lab.</p>

<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab12<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab12 --output table<br/>
</div>

<p>Once the job has completed, investigate the number of partitions before and after the repartitioning in <a href="https://eu-west-1.console.aws.amazon.com/cloudwatch/" target="_blank">Amazon CloudWatch</a>.</p>

<h1>Lab 13 - Write large data sets to DynamoDB using partioning</h1>
<p> Now that we have learned a way to parition our data, we will be loading data in to DynamoDB using multiple paritions. Multiple partitions will allow AWS Glue to run multiple workers in parallel and thus speed up the write process. We will be loading a fairly big data set of 1M rows. Start by running the job with the default of just 2 workers.</p>

<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab13<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab13 --output table<br/>
</div>

<p>
Note the timings. Then up the number of workers to 6, on the job details tab. Run the job again and note the different timings. Feel free to play around with the number of workers and partitions if you have spare time.
</p>

<h1>Lab 14 - Create an AWS Glue Crawler for DynamoDB</h1>
<p>
    For the next couple of labs, you will not be using a pre-created Glue job and script. Instead you will be using the AWS Console UI to complete these labs. This will give you a feel for how the UI experience is for Glue.</p>
<p>
    A crawler will crawl a given data source (i.e. DB, S3 bucket or similar). During the crawling it will identify the data schemas, as well as partitions, keys and other meta data.
    Create glue DB. In this lab we will be creating a <a href="https://docs.aws.amazon.com/glue/latest/dg/crawler-running.html" target="_blank">Glue crawler</a> for the DynamoDB table that we loaded trades to in a previous lab (lab 10).
</p>

<p>Open the 'burger' menu on the left side. Navigate to 'Crawlers' and create a new crawler.</p>
<img src="images/add-crawler.png">
<p>Choose DynamoDB as source, and choose the table 'acc-day-glue-trades'. Ignore the S3 crawl options and continue through the wizard with the 'DynamoDB' option.</p>

<img src="images/crawler-add-data-store.png">

<p>Use the same IAM role as in previous labs. You can check the name on the Glue job details tab, if you have not noticed. Choose to run the crawler 'on demand'. As we have not yet created an AWS Glue database, we will need to create one to contain our table schemas. Create a Glue database named 'acc-day-glue-database' and choose to prefix tables with 'ddb_trades_' so that they can be identified easily.</p>

<img src="images/crawler-add-database.png">

<p>Leave everything else as is. Finish the wizard. The crawler should now show on the list of crawlers page. Select the crawler and choose 'Run crawler' or click the 'Run it now' button inside the green banner. Wait for the crawler to complete the run. Click the '1' under the 'Tables added' column, and investigate the table meta data.</p>

<img src="images/crawler-run.png">

<h1>Lab 16 - Create an AWS Glue Crawler for MySQL</h1>
<p>This time we will be creating a crawler for MySQL, which is a bit different from the crawler we just created for DynamoDB. When creating a crawler for MySQL, or any other relational database that has the concept of schemas and multiple tables, you can decide whether you want to crawl just a specific table, a specific schema or the entire database. In this lab we will be crawling the entire database as it has only one table for now. We will be adding two more tables before crawling the database.</p>
<p>Start by connecting to the MySQL database through the Cloud9 termnial. Make sure that the <a href="https://eu-west-1.console.aws.amazon.com/vpc/home?region=eu-west-1#securityGroups:" target="_blank">Secruity Group</a> for Cloud9 has access on the MySQL port for the Security Group for MySQL, if you did not already do this in 'Lab6'. If you forget this, your connection attemp will time out. Run the code below to connect to the database. Remember to replace the values with the right information for your database instance. You can find these using <a herf="https://eu-west-1.console.aws.amazon.com/secretsmanager" target="_blank">Secrets Manager</a>.
</p>

<div class="code">
    mysql -h [db-prefix].eu-west-1.rds.amazonaws.com -u [dbuser] -p[dbPW] tradedb<br/>
</div>

<div class="code">
    <br/>
    create table cpty (<br/>
    cpty_id int PRIMARY KEY NOT NULL AUTO_INCREMENT,<br/>
    short_name varchar(30),<br/>
    full_name varchar(100),<br/>
    external_id varchar(50),<br/>
    region varchar(20)<br/>
);<br/>
<br/>
insert into cpty values(null, 'COMP_A', 'COMPANY A', 'F-312331', 'EUROPE');<br/>
insert into cpty values(null, 'COMP_B', 'COMPANY B', 'IOA-90005', 'US');<br/>
insert into cpty values(null, 'COMP_F', 'COMPANY F', 'PLJJH-22-KRTT', 'AFRICA');<br/>
insert into cpty values(null, 'COMP_G', 'COMPANY G', 'F-222322', 'EUROPE');<br/>
<br/>
create table counterparty (<br/>
    counterpartyid int PRIMARY KEY NOT NULL AUTO_INCREMENT,<br/>
    shortname varchar(30),<br/>
    fullname varchar(100),<br/>
    extid varchar(50)<br/>
);<br/>
<br/>
</div>

<p>Just like we did with the DynamoDB crawler, go through the AWS Glue Crawler wizard, but this time you will need to choose JDBC as the datastore. Also you will need to use the connection we created earlier, to reach MySQL as it runs inside a VPC, and you will need to choose the correct IAM Role and Security Group. See if you can figure it out yourself. Below is a screenshot that will guide you a bit. If you get stuck ask for help.</p>

<img src="images/crawler-mysql-add-data-store.png" alt="">

<p>Once you complete the wizard, the crawler should will shot on the crawlers page. Select the crawler and choose 'Run crawler' or click the 'Run it now' button inside the green banner. Wait for the crawler to complete the run. Click the '2' under the 'Tables added' column, and investigate the table meta data. As we are crawling the entire database, Glue will find two tables as we added two more tables. The third table is not crawled as it has no data yet.</p>

<h1>Lab 17 - Create a job using the UI</h1>
<p>In this lab we will be investigating how to use the AWS Glue UI. We will create a job using the UI for transferring data from the source table 'cpty' to the target table 'counterparty'. The two tables were already created in the previous exercise.<p>

<p>
    In this exercise it is time to test some of the things you have learned so far. See if you can figure out how to create the job yourself using the UI.<br/><br/>
    Create a new Glue visual job using the wizard. Name your job 'acc-day-glue-lab17'.<br/><br/>
    As we will be working with bookmarks in the next lab, for this lab we will disable bookmarks. In the job settings tab, scroll down to 'Bookmarks' and choose the option 'Disable'. While on the settings tab also ensure that the number of workers are set to 2, timeout to 20 minutes, and retry attempts to 0. Scroll down to the advanced section and ensure that you add the "acc-day-glue-vpc-conn" as a connection. Without this your connection to the database will time out. Finally specify the role for the job being the "acc-day-glue-execution-role".<br/><br/>
    You will be using MySQL as both source and target, as they are already crawled both endpoints will already exist, and thus be known to Glue and the Glue UI. This means that you can choose them from the Glue database and table drop downs instead of specifying all the details again.<br/><br/>
    Also note that the column names in the two tables are not the same. You will need to make sure that the mapping takes care of this.</p>
<p>
    When you are done configuring the job, save it and run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab17<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab17 --output table<br/>
</div>

<p>At this point you can login to the MySQL database like we did previously in the Cloud9 terminal, if you are not still logged in. Both tables should now contain the same rows. Run the queries below to verify that the data transfer was successful.</p>

<div class="code">
    select * from cpty;<br/>
    select * from counterparty;<br/>
</div>

<h1>Lab 18 - Run a delta job using bookmarks (optional)</h1>
<p>
    Bookmarks works differently depending on your source data store. In this lab we will investigating how to use bookmarks with a relation database as a source data store. In this lab you will be using the job that you created in the previous lab as a starting point. Start by cloning the job, you can do this directly using the UI. Rename the job to 'acc-day-glue-lab18'. Next change the bookmark settings in the job details tab to 'Enabled'. Run the new job and verify the contents of the target table has not changed. As we have defined primary keys for the tables, AWS Glue will use these to keep track of what has been transferred already.
</p>
<p>
    Insert a few more rows in the source table. Use the MySQL terminal as you did previously and run the commands below.
</p>

<div class="code">
    insert into cpty values(null, 'COMP_Y', 'COMPANY Y', '00888-6655PP', 'ASIA');<br/>
    insert into cpty values(null, 'COMP_X', 'COMPANY X', '00334-110I', 'ASIA');<br/>
</div>

<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

<div class="code">
    aws glue start-job-run --job-name acc-day-glue-lab18<br/>
    aws glue get-job-runs --job-name acc-day-glue-lab18 --output table<br/>
</div>

<p>Verify the contents of the target database. It should now also contain the rows that we just added to the source table. This is a nice way to transfer data in chunks, meaning that you could choose to transfer data several times a day, daily, weekly or at any schedule that matches your usage pattern.</p>

<h1>Lab 19 - Resetting bookmarks (optional)</h1>
<p>In this final lab we will be testing the reset bookmark functionality. You have the option to reset bookmarks for a given job at any time. This will erase all track of progress for the Glue job. Try to reset the job in the previous lab. Run the job and notice what happens.
</p>

<p>Congratulations, you made it through all the basic labs. Good work and please continue to do great things with AWS Glue.</p>
<img src="images/badge_2.jpg">

<div class="footer">
    &copy; 2022, Amazon Web Services, Inc. or its affiliates.
</div>
</body>
</html>