<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="index.css">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js"></script>
    <script src="index.js"></script>
</head>
<body>

<div class="container-fluid">           
<div class="row" style="height: 6rem; width:100%; background-color: #232F3E;">
    <div class="col-md-auto" style="line-height: 6rem;"><img src="images/aws_tiny_dark.png"></div>
    <div class="col-md-auto">
    <div style="color: #ffffff; font-size: 1.8rem; line-height: 6rem;" id="prerequisites">AWS Glue - Introduction</div>
    </div>
</div>

<h1>Prerequisites</h1>
<p>
If not already done, please complete the <a href="">Getting Started</a> lesson first. Next log in to <a href="https://eu-west-1.console.aws.amazon.com/cloud9/" target="_blank">AWS Cloud9</a>. The Python code for all labs are available via the GitHub link that was cloned earlier.
</p>

<div class="code">
cdk deploy --all
</div>

<p>You can roughly expect the following runtimes for the different stacks that are deployed as part of this process.</p>
<ul>
<li>Network Stack: 200 secs</li>
<li>Security Stack: 30 secs</li>
<li>Data Stack: 616 secs</li>
<li>Compute Stack: 57 secs</li>
</ul>

<h1 id="lab-0">Lab 0 - Setup</h1>
<p>
If not already done, start by logging in to <a href="https://eu-west-1.console.aws.amazon.com/cloud9/" target="_blank">AWS Cloud9</a>. Next we will first ensure that the AWS credentials used are the right ones, and next we will start to copy some sample data to their respective S3 buckets.
</p>

<div class="code">
# ensure you are using Event Engine credentials<br/>
aws sts get-caller-identity<br/>
aws s3 ls | grep input<br/>
echo please type &quot;trade-input-bucket-name&quot;<br/>
read tradeInputBucket<br/>
cd ~/environment/git/aws-glue/python/trade-import<br/>
python generate-large-data-file.py<br/>
python generate-huge-data-file.py<br/>
aws s3 cp ~/environment/git/aws-glue/sample-data/trade/fx-trades.json s3://$tradeInputBucket/<br/>
aws s3 cp ~/environment/git/aws-glue/sample-data/trade/fx-trades.csv s3://$tradeInputBucket/<br/>
aws s3 cp ~/environment/git/aws-glue/sample-data/trade/fx-trades-large.csv s3://$tradeInputBucket/<br/>
aws s3 cp ~/environment/git/aws-glue/sample-data/trade/fx-trades-huge.csv s3://$tradeInputBucket/<br/>
aws s3 ls | grep driver<br/>
echo please type &quot;driver-bucket-name&quot;<br/>
read driverBucket<br/>
cd /tmp/<br/>
wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.25/mysql-connector-java-8.0.25.jar<br/>
aws s3 cp mysql-connector-java-8.0.25.jar s3://$driverBucket/<br/>
</div>

<p>Once all files has been copied to S3, verify that the files are in the buckets using either the AWS CLI or the <a href="https://eu-west-1.console.aws.amazon.com/s3/" target="_blank">Amazon S3 Console</a>.</p>

<ul>
<li>Create target table &quot;trade&quot; in MySQL (for performance reasons).</li>
</ul>

<h1 id="lab-1">Lab 1 - Creating a basic job</h1>
<p>
Run an &quot;Empty&quot; job just with context and logging.<br/>
You can run it either via the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or the AWS CLI using Cloud9. Try both.<br/>
</p>

<div class="code">
aws glue list-jobs<br/>
aws glue start-job-run --job-name acc-day-glue-lab1<br/>
aws glue get-job-runs --job-name acc-day-glue-lab1 --output table<br/>
</div>

<p>Note the output logs in CloudWatch as well as the timings for the run.</p>

<h1>Lab 2 - Read secret from AWS Secrets Manager</h1>
<p>
In this second lab we will be reading a database secret from AWS Secrets Manager. Navigate to <a href="https://eu-west-1.console.aws.amazon.com/secretsmanager/" target="_blank">AWS Secrets Manager</a> to view the secret as well as the secret values.
</p>

#runjob

<h1>Lab 3 - Read CSV data from Amazon S3</h1>
<p>
Now that we have the basics in place, we will actually start to read data from an S3 object store. AWS Glue has the concept of <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html" target="_blank">DynamicFrames</a> which is a more dynamic version of the more well known Python DataFrames. In most cases, reading data in to a DynamicFrame will be the first actual step in any Glue ETL job.
</p>

#runjob

<h1>Lab 4 - Trim data in source data frame</h1>
<p>
Often source data comes in all shapes and sizes and can be filled with unexpected characters, whitespaces and much more. In the previous lab, did you notice some potential issues with the input file? Actually there were whitespaces in front of most of the values, which would cause problems as part of the type conversion process. You can verify this by looking at the JSON print out in <a href="https://eu-west-1.console.aws.amazon.com/cloudwatch/" target="_blank">Amazon CloudWatch</a>. In this lab we will make sure that all whitespaces are trimmed.
</p>

#runjob

<h1>Lab 5 - Apply mappings</h1>
<p>
    After reading the data we want to map it to the target schema. This is done using the apply() method of the <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-transforms-ApplyMapping.html" target="_blank">ApplyMapping</a> class. The ApplyMapping class is one of the AWS Glue classes in the <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-transforms.html" target="_blank">Transforms</a> namespace. Feel free to investigate the other classes in this namespace when time allows.
</p>

#runjob

<h1>Lab 6 - Write to target Amazon RDS MySQL DB</h1>
<p>
So far we have been reading data from Amazon S3, and we have trimmed/prepared and mapped the data to the target database schema. In this lab we will actually be writing the data to the target data store. We will be writing to Amazon RDS for MySQL, which is a managed MySQL compatible relational database offering from AWS.
</p>

#runjob

<p>
Once the job has completed, connect to the MySQL database instance, using the MySQL CLI from Cloud9. Please ensure that you have configured the RDS security group to allow inbound connections from Cloud9 for the MySQL port, as described earlier. The job will only insert a few records in this lab, so it is safe to select all contents from the table using the code below.
</p>

#code

<h1>Lab 7 - Read JSON data from S3</h1>
<p>
    Reading JSON data from S3 is very similar to reading CSV data. The main difference in the code is the format parameter of the fromOptions() method which changes value from csv to json. A few other csv specific options are removed, and a few are added. We will not be preparing or mapping the data to anything in this lab, but we will simply read the data and investigate the CloudWatch logs.
</p>
<p>
    Run the job using the <a href="https://eu-west-1.console.aws.amazon.com/glue/" target="_blank">AWS Glue Console</a> or using the AWS CLI as shown below.
</p>

#runjob

<p>
    Once the job has completed open #cwlink and notice the output from the DynamicFrame that has been read.
</p>

<h1>Lab 8 - Import large data set</h1>
<p>
Often when dealing with ETL jobs you will be working with thousands or even millions of rows. AWS Glue has many features to support large data sets. In the coming labs we will be focusing on some of those features and capabilities. Some are AWS Glue specific and other are related to the Python and PySpark language and framework.</p>
<p>
In this lab we will load 1M rows from a CSV file, and investigate the runtime metrics and logs produced by the job. This will give us an idea of the speed that you can expect from AWS Glue with the given settings. Also note the job details for the job, which specifies both the number of workers and the instance type used, being to of the most critical components when right sizing AWS Glue ETL jobs.
</p>

#runjob

<h1>Lab 9 - import huge data set (SKIP or let timeout)</h1>
Import large data set 20M+ rows.<br/>
Will most likely timeout as is.<br/>
Try to modify table.<br/>
Add Cloud9 to RDS security group inbound rule.<br/>
mysql -h .rds.amazonaws.com -u xx -pXX tradedb<br/>

<h1>Lab 10 - Writing to Amazon Dynamodb</h1>
<p>
For decades most databases have been relational databases or so called RDBMS. For the past years many new types of databases has emerged, including NoSQL and KeyValuePair databases. The ability to scale horizontally, as vertical scaling has limits and can be hard to right size, and the struggle to keep relational database schemas up to date and in sync with code and APIs are some of the main reasons why these database types are becoming more and more popular.
Write data to DynamoDB instead of MySQL
</p>

#runjob

<h1>Lab 11 - Write large data sets to Amazon Dynamodb</h1>
<p>

</p>

#runjob

<h1>Lab 12 - Using multiple partitions</h1>
<p>

</p>

#runjob

<h1>Lab 13 - Write large data sets to DynamoDB using partioning</h1>
<p>
Run job.
Up number of glue workers to match num partitions (6).
Run job again.
Note diff if any.
</p>

#runjob

<h1>Lab 14 - Create an AWS Glue Crawler for DynamoDB</h1>
<p>

</p>
Create glue crawler for DynamoDB table.
Create glue DB.
Run crawler on demand.

<h1>lab 16 - Create an AWS Glue Crawler for MySQL</h1>
<p>
When creating a crawler for MySQL or any other relation database that has the concept of schemas and multiple tables, you can decide whether you want to crawl just a specific table, a specific schema or the entire database. In this lab we will be crawling the entire database as it has only one table for now. We will be adding two more tables before crawling the database.
</p>

<ul>
<li>Create counterparty table.</li>
<li>Insert data into counterparty table.</li>
<li>Create cpty table.</li>
<li>Create crawler for RDS MySQL DB (tradedb/%)</li>
</ul>

<h1>Lab 17 - Create a job using the UI</h1>
<ul>
    <li>Create UI job transferring data from counterparty to cpty</li>
    <li>Disable bookmarks.</li>    
</ul>

<h1>Lab 25 - Run a delta job using bookmarks (optional)</h1>
<p>
    Bookmarks works differently depending on your source data store. In this lab we will investigating how to use bookmarks with a relation database as a source data store.
</p>
<ul>
    <li>Enable bookmarks</li>
    <li>Read existing data</li>
    <li>Verify that data is written to target</li>
    <li>Add more data to source</li>
    <li>Read data again</li>
    <li>Verify that only new data is read/written to target</li>
</ul>

<h1>lab 26 - reset bookmark (optional)</h1>
<ul>
    <li>Reset bookmark</li>
</ul>

<div class="footer">
    &copy; 2022, Amazon Web Services, Inc. or its affiliates.
</div>
</body>
</html>