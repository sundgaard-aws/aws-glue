<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="index.css">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js"></script>
    <script src="index.js"></script>
</head>
<body>

<div class="container-fluid">           
<div class="row" style="height: 6rem; width:100%; background-color: #232F3E;">
    <div class="col-md-auto" style="line-height: 6rem;"><img src="images/aws_tiny_dark.png"></div>
    <div class="col-md-auto">
    <div style="color: #ffffff; font-size: 1.8rem; line-height: 6rem;" id="prerequisites">AWS Glue - Introduction</div>
    </div>
</div>

<h1>Advanced lessons with AWS Glue</h1>
<p>
Before beginning these lessons, please ensure that all AWS Glue intro lessons has been completed, or that you have a solid understanding of AWS Glue at this point.
</p>

<h1>Lab 29 - Write partitioning with Amazon S3 (s3=>s3 - no parition key)</h1>
<p>
    In this exercise we will be creating partitions in an S3 target bucket. The source will be a CSV file containing 20M rows of trade data.<br/>
    We will not be specifying a partition key in this lab, but AWS Glue are paritioning even though.<br/>
    Note how data is structured in separate partitions (multiple files) in the target folder.<br/>
    Open one or more of the files to verify the contents.<br/>
</p>

<h1>Lab 30 - Write partitioning with Amazon S3</h1>
<p>
    In this exercise we will be creating partitions in an S3 target bucket. The source will again be a CSV file containing 20M rows of trade data.<br/>
    This time we will specify "ccy" as the parition key on the target bucket.<br/>
    Run the job and check the status.<br/>
    Navigate to the s3 console and find the right bucket.<br/>
    Note how data is structured in first divded in to multiple partitions, one per ccy, and next it has multiple files per partition.<br/>
    Open one or more of the files to verify the contents.<br/>
</p>

<h1>Lab 31 - Merging files in partitions with Amazon S3</h1>
<p>
    * AWS Glue is based on Apache Spark which again is optimized for big data and massive parallelized processing. When working with files it is common, for this reason, that both input and output files are split in to multiple files.<br/>
    * We will continue from the previous exercise. As you noted in the previous lab, each of the target partition folders have multiple files within them. At times it is necessary to merge the files in a target folder to a single file. This can be done with some simple scripting.<br/>
    * Go to the scripting tag of the lab.</br>
    * Try to understand the details of the loop that is doing the merge and writing the consolidated files to s3.</br>
    * Run the job and verify the status.</br>
    * Go to the s3 console and verify that there is a "merged.csv" file in each partition folder.</br>
    * Open one or more of the merged files to verify that the contents are correct.</br>
</p>

<h1>Lab 32 - Nested write partitions with Amazon S3 (s3=>s3, huge by date and ccy)</h1>
<p>
    There are several reasons why you would want to use partitioning. It can be used to organize and parallelize data, but it is also useful to filter or read just a specific partition to quickly narrow down the wanted data set.<br/>
    Run the job and check the status.<br/>
    Navigate to the s3 console and find the right bucket.<br/>
    Note how data is first divded in to multiple partitions, first by "trade_date", then by "ccy", and next it has multiple files per partition.<br/>
    In this exercise we will create nested partitions, meaning that data will first be partitioned by "trade_date" and next by "ccy".<br/>
    What we did in this exercise was to create another data dimension, and another level of nested paritions. Investigate how the files and "folders" generated by this lab, are structed in the s3 bucket. You can quickly imagine that if you want to filter data by a parition consisting of "trade_date" or "trade_date" and "ccy", performance will be boosted significantly. Keep in mind though that the deeper partitions are nested, the fewer are the use cases for those data sets, but performance is boosted equivalently.<br/>
</p>    

<h1>Lab 33 - Create Aurora PostgreSQL DB and Crawler</h1>
<p>    
    * So far we have only been using S3, MySQL and DynamoDB. We will now add a PostgreSQL compatible database using Amazon Aurora Serverless.<br/>
    * Using the Cloud9 IDE Open the file at "iac/lib/data-stack.ts".<br/>
    * Uncomment the line with the text "this.createRDSAuroraPostgreSqlDB" by removing the two slashes.<br/>
    * Run the command below to deploy PostgreSQL database:<br/>
    <div class="code">
        npx cdk deploy *<br/>
    </div>
    * Wait for the deployment to finalize.<br/>
    * Install psql by adjusting the replaceable sections and running the following commands in a Cloud9 terminal session:<br/>
    <div class="code">
        # start cloud shell<br/>
        sudo yum install -y postgresql-libs.x86_64 postgresql.x86_64<br/>        
    </div>
    * Connect to the database in Cloud9 by running the command below:<br/>
    <div class="code">
        aws secretsmanager get-secret-value --region eu-west-1 --secret-id [replace-with-your-secret-id]<br/>
        psql --host acc-day-glue-tm-aurora-cluster.cluster-[replace].eu-west-1.rds.amazonaws.com --user postgres<br/>
    </div>
    * When prompted for the pw, navigate to Secrets Manager and find the secret for the new Aurora DB.<br/>
    * Paste the pw in the psql prompt<br/>
    * Once connected to the database prompt, switch to the proper DB<br/>
    <div class="code">
        \connect tm_aurora_db<br/>
    </div>
    * Next create a trade table by running the commands in the file "create-trade-table-psql.sql".<br/>
    * Next navigate to crawlers<br/>
    * Create a new Glue connection using JDBC <br/>
        * Name it “aurora-glue-conn”<br/>
        * Use the following string for the JDBC part and replace where needed (such as [replace-with-your-settings]):<br/>
    <div class="code">
        jdbc:postgresql://acc-day-glue-tm-aurora-cluster.cluster-[replace-with-your-settings].eu-west-1.rds.amazonaws.com:5432/tm_aurora_db<br/>
    </div>
    * Create a new Glue crawler<br/>
        * Specify crawler name “aurora-crawler”<br/>
        * Create a Glue database and name it “all-dbs”<br/>
        * Specify the connection “aurora-glue-conn” in the connection drop down<br/>
    * Complete the wizard with default settings and finally choose to run the crawler.<br/>
    * Wait for crawler to complete, and verify that it has found the "trade" table.<br/>
</p>

<h1>Lab 34 - Custom transforms (using Glue UI)</h1>
<p>
    AWS Glue has both UI and scripting capabilities. In some cases you may want to use the UI to get started quickly and to get an idea of how your script could look like. When using the UI, even though many functions are available out of the box, you <br/>may find yourself missing some functionality.<br/>
    For this reason you can use "custom transforms" in AWS Glue. This will allow you to write custom transformation snippets while still maintaining the UI state of a job.<br/>
    <p>Create a new UI Glue Job</p>
    <img src="images/lab34-1.png" />
    <p>Name it "acc-day-glue-lab-34".</p>
    <img src="images/lab34-2.png" />
    <p>Choose the file "fx-trades-large.csv" from the trade input bucket.</p>
    <img src="images/lab34-3.png" />
    <p>Change all types in the output schema to "string"</p>
    <img src="images/lab34-4.png" />
    <p>Add a custom transformation from the "Transform" drop down.</p>
    <img src="images/lab34-5.png" />
    <p>Also add a "Select From Collection" element, and adjust the canvas to look like the one below.</p>
    <img src="images/lab34-6.png" />
    <p>Click the custom transform element and paste the code from the file "custom-transform.py" just below the function definition (exclude the function definition from the file) and click "Apply".</p>
    <img src="images/lab34-7.png" />
    <p>Name it "acc-day-glue-lab-34".</p>
    <img src="images/lab34-8.png" />
    <p>Name it "acc-day-glue-lab-34".</p>
    <img src="images/lab34-9.png" />
    <p>Name it "acc-day-glue-lab-34".</p>
    <img src="images/lab34-10.png" />
    <p>Name it "acc-day-glue-lab-34".</p>
    <img src="images/lab34-11.png" />
    <p>Name it "acc-day-glue-lab-34".</p>
    <img src="images/lab34-12.png" />    
    <p>Connect to psql prompt in Cloud9 just as you did in "lab 33" and run the code lines below and verify that the "trade" table is empty.</p>
    <img src="images/lab34-14.png" />
    <div class="code">
        select count(1) from trade;
    </div>
    <p>Run the job and wait for completion. When completed run the sql statement again to verify that all rows has been inserted.</p>
    <img src="images/lab34-13.png" />
    <div class="code">
        select count(1) from trade;
    </div>
</p>    

<h1>Lab 35 - Using include libraries</h1>
<p>
    The more ETL jobs you write, the more code and functionality you will find yourself repeating across jobs. To avoid situations where code maintenance becomes a burden, it is often helpful to generalize your code and wrap it as a reusable function.<br/><br/>
    Functions can be placed in modules (for Python), which can be either zip archive files or simply .py files. You can maintain and store these files as with any other code in your code repository of choice. As part of your CI/CD deployments you can make these modules available on S3, and include them in any of your AWS Glue jobs.<br/><br/>
    In this exercise we will be moving some of the general functions that we have created in previous exercises to a separate Python module. We will then upload the module to S3 and include them in a job.<br/><br/>
    Got to the "Script" tab of the job.<br/><br/>
    Remove the section defining the two first functions as indicated by the dotted lines.<br/><br/>
    While still on the "Script" tab add the following to lines as part of the top imports.<br/>
    <div class="code">
        from generic_glue_functions import getSecret<br/>
        from generic_glue_functions import getParameter<br/>
    </div>
    <br/>
    It should look like the image below.<br/>
    <img src="images/lab35-5.png" alt="" />
    <br/><br/>
    Go to the "Job details" tab.<br/><br/>
    Scroll down to the "Advanced" section and expand it. Scroll down to the "Python library path" field.<br/><br/>
    Open the S3 Console in another tab.<br/><br/>
    Download the <a href="https://github.com/sundgaard-aws/aws-glue/raw/main/python/trade-import/generic_glue_functions.py">Python module</a> to your local computer (right click the link and choose save link as).<br/><br/>
    Go to the "acc-day-glue-input-*" bucket.<br/><br/>
    Create a new folder called "libs".<br/><br/>
    <img src="images/lab35-1.png" alt="" />
    Upload the file "generic_glue_functions.py" to the "acc-day-glue-input-*" bucket in the "/libs/" folder by dragging in the file from your local download folder.<br/><br/>
    <img src="images/lab35-2.png" alt="" />
    Click the link of the newly uploaded file object and copy the full S3 path from the S3 Console.<br/><br/>
    <img src="images/lab35-3.png" alt="" />
    Paste the full S3 path to the S3 python module that you uploaded previously, and save the job.<br/><br/>
    <img src="images/lab35-4.png" alt="" />
    Run the job.<br/><br/>
    Verify that the job succeeded.<br/><br/>
</p>
<p>
    Now you know how you can easily make reusable functions with AWS Glue.
</p>

<h1>Lab 36 - Using AWS Glue - Interactive Sessions (embedded Jupyter notebooks) - NEW!</h1>
<p>
    Create a new AWS Glue job by selecting the "Notebook" option at the jobs creation page.<br/><br/>
    <img src="images/lab36-1.png" alt="" />
    Name it "acc-day-glue-lab36".<br/><br/>
    <img src="images/lab36-2.png" alt="" />
    Wait for the notebook to finish start up.<br/><br/>
    <img src="images/lab36-3.png" alt="" />
    Collapse the top description by clicking the blue bar on the side, and paste in the code from <a href="https://github.com/sundgaard-aws/aws-glue/raw/main/python/trade-import/generic_glue_functions.py">this file</a> in a new cell. You can create a new cell by clicking the "+" next to the "Save" icon, or by pressing ALT+ENTER in an existing cell. When done run the cell by placing the cursor in the cell and hit SHIFT+ENTER.<br/><br/>
    <img src="images/lab36-4.png" alt="" />
    We are actually reading from a file from a previous lab and printing that. At this point you can change the input file or start manipuliting data just by adding your logic in new cells (or the same for that matter), and running them. We will be working more with Notebook based jobs in some of the remaining exercises, but feel free to work with and transform, clean and manipulate data further using this notebook before you continue.
</p>    

<h1>Lab 37 - Aggregate amounts by trader using UI</h1>
<p>
    Create a new blank job<br/>
    Read from the AWS Glue “s3-large" table that we created by crawling an s3 bucket earlier<br/>
    Choose the "aggregate” function under the “Transform” menu<br/>
    Choose "trader_id" as the group by<br/>
    Choose "trade_amount" as sum<br/>
    Run the job.<br/>
    Verify that the run is successful.<br/>
    At this time you are not able to view the final output, we will improve that in the coming two labs.<br/>
</p>

<h1>Lab 38 - Aggregate amounts by trader using notebook</h1>
<p>
    Create a new notebook job and copy the various parts of the script from the previous lab to individual cells in the notebook. <br/>
    Run them step by step and note the individual timings. <br/>
    Try to change the grouping and sum columns to get a feel for what is possible and the related timings.<br/>
</p>

<h1>Lab 39 - Create aggregation reports</h1>
<p>
    * Clone the job from Lab 37, rename it to create-aggregation-reports. Choose “edit script“. <br/>
    * Aggregate amounts by “trader_id” in one stream, and by “currency” (ccy) in another stream. <br/>
    * Write the resulting data frames to two individual CSV files in the S3 “xxx-trade/reports/” bucket. <br/>
    * Name them “aggregated-amounts-by-trader“  and “aggregated-amounts-by-ccy“.<br/>
    * After job completion download and open the files using Excel.<br/>
    * Convert the two columns from text to columns and specify comma as the separator<br/>
    * Add filtering.<br/>
    * Analyze the data to verify that it looks as you would expect based on the input.<br/>
</p>    

<h1>Lab 40 - Generate PII</h1>
<p>
    In this exercise we will be generating PII data. We are simulating a common situation for GDPR, where deletion of information is requested by an individual. We will be generating 1M rows containing synthetic bank accounts information and Danish CPR numbers, which qualifies as PII and GDPR data.<br/>
    Run the job "Lab 40"<br/>
    Check the status of the job, and note the actual duration of the "true" processing in the CloudWatch logs.<br/>
    Verify that the file "synthetic-pii.csv" has been generated in the "trade" bucket.<br/>
    Download the file from the S3 console and open it in Notepad++, Excel, VSCode or any editor that supports line count<br/>
    Verify that it contains exactly 1M rows if you deduct headers and empty lines at the end<br/>
</p>

<h1>Lab 41 - Cleanup PII</h1>
<p>
    In this lab we will be removing selected individual PII as explained in the previous lab.
    Run the job<br/>
    Check the status of the job, and note the actual duration of the "true" processing in the CloudWatch logs.<br/>
    Verify that the file "synthetic-pii.csv" has been generated in the "trade" bucket.<br/>
    Download the file from the S3 console and open it in Notepad++, Excel, VSCode or any editor that supports line count<br/>
    Verify that it contains exactly 999997 (1M-3) rows if you deduct headers and empty lines at the end<br/>
</p>

<h1>Lab 42 - Create table for performance metrics</h1>
<p>
    Performance and metrics are an important part of understanding your ETL jobs. AWS Glue shows a number of metrics related to runtime, status, CPU, memory and more. Compute nodes backing AWS Glue will only run when used to reduce costs. This means that for short running jobs (a few minutes or seconds), the warm up time has a big impact on the total time spent. In order to better understand what takes time, we can easily add more metrics to CloudWatch or any place that we desire to log such metrics.<br/>
    In the following labs we will be gathering such metrics for some of the jobs that we previously created.<br/>
    Let us start by creating a table for holding the metrics in the postresql database.<br/>
    Login to psql as previously and create a table as per the code below.<br/>
    <div class="code">
        create table perf_metrics (
        entry_id int PRIMARY KEY NOT NULL GENERATED ALWAYS AS IDENTITY,
        job_name varchar(80),
        duration decimal,
        entry_time timestamp
        );<br/>
    </div>
</p>

<h1>Lab 43 - Collect performance metrics</h1>
<p>
    We now have our table in place, and it is time to collect some metrics from jobs run in previous labs.<br/>
    Pick five or more jobs that you found pariculary interesting.<br/>
    In the script part of those jobs add a call to startStopwatch() and stopStopwatch() where you find it appropriate. As a rule of thumb you would like to surround the time consuming blocks of code. If you are unsure, you can add the first call just after function definitions and the stop just before the job.commit statement.<br/>
    <div class="code">
        startStopwatch()<br/>
        stopStopwatch()<br/>
    </div>
</p>


<h1>Lab 50 - Performance comparison using notebook</h1>
<p>
    This is a freestyle bonus exercise. If you still have time left, it is time to see the power or AWS Glue notebooks and interactive sessions. You can reuse the notebook that we created in "lab38", or you can create a new one, and name it "lab50".<br/>
    Once the notebook is ready to use, find interesting code snippets from previous labs on the script tab of those labs, and copy the snippets to individual cells on the notebook.<br/>
    Note the timings of the individual snippets, either using your own stopwatch or by using the stopwatch methods that we used in "lab43".<br/>
    While experimenting an creating code for new jobs, you will most likely find it much faster to use a notebook. The main reason for this, is that you do not have to wait for instances starting up every time you run a seperate code snippet. Also you are able to jump back and fort between cells, and modify code on the fly.<br/>
    Try out as many things as you can until you are out of time.<br/>
    Also be aware that you will have to use "magics" in order to configure the VPC connection that we have used in many of the previous labs. Can you figure out how to configure that?<br/>
</p>





<p>Congratulations, you made it through all the labs. Good work and please continue to do great things with AWS.</p>

<img src="images/badge_2.jpg">

<div class="footer">
    &copy; 2022, Amazon Web Services, Inc. or its affiliates.
</div>
</body>
</html>